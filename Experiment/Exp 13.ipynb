{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676cecaa-e249-4db9-8d59-0c652039f2b9",
   "metadata": {},
   "source": [
    "# Experiment 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d5e86-2ed4-489f-b954-b86f9f0b0c53",
   "metadata": {},
   "source": [
    "# Write a NLP Program to demostrate following tasks \n",
    "## a. Tokenization, Removal of stop words, Punchuation, POS & NER Tags \n",
    "## b. Bag of Words, TF-IDF Vectorisation & Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcda90-bc62-45e9-a2ad-3cb4499ca47f",
   "metadata": {},
   "source": [
    "## (a) Text Preprocessing\n",
    "\n",
    "Tokenization → Break text into words.\n",
    "Example: \"Apple is looking...\" → [\"Apple\", \"is\", \"looking\", ...].\n",
    "\n",
    "Stop word & punctuation removal → Remove useless words like “is, at, the” and symbols.\n",
    "Example: [\"Apple\", \"looking\", \"buying\", \"startup\"].\n",
    "\n",
    "POS Tagging → Identify word roles.\n",
    "Example: \"Apple/NNP\" (noun), \"looking/VBG\" (verb).\n",
    "\n",
    "NER → Identify names of people, places, companies, money, etc.\n",
    "Example: \"Apple\" → ORG, \"Elon Musk\" → PERSON, \"$1 billion\" → MONEY.\n",
    "\n",
    "## (b) Feature Extraction\n",
    "\n",
    "Bag of Words → Counts how many times each word appears.\n",
    "Example: [\"Apple\", \"Tesla\"] → [1,1].\n",
    "\n",
    "TF-IDF → Like BoW, but also considers importance of words (rare words = more weight).\n",
    "\n",
    "N-Grams → Sequences of words (e.g., bigram = 2 words).\n",
    "Example: \"Elon Musk\" is treated as a phrase, not two separate words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf8182-87f0-4ebe-a331-0ac3e6fc9461",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061bc083-2c74-4c44-a6d9-4e36f98ffac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe43db-838d-47e6-9219-fb4b5ff205a8",
   "metadata": {},
   "source": [
    "## Download required NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f7eed1-8125-4a36-893c-ef6dabc4c86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83edd509-ac98-4e5f-935d-bd22b1e5af59",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11112ff-4420-46a5-a6cf-2384d78f5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Elon Musk is the CEO of Tesla\n"
     ]
    }
   ],
   "source": [
    "text = \"Elon Musk is the CEO of Tesla\"\n",
    "print(\"Original Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7263b4-292a-4590-a3dc-3d867d41eb9a",
   "metadata": {},
   "source": [
    "# A: Tokenization, Stop Words & Punctuation Removal & POS & NER Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b368b7c-1339-4a9c-aeb4-4388f1901157",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3aa8ec5-c73c-4934-81eb-668280b1ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokens ---\n",
      " ['Elon', 'Musk', 'is', 'the', 'CEO', 'of', 'Tesla']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"\\n--- Tokens ---\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daaa46a-3be5-4da5-9888-c649c51e0f6f",
   "metadata": {},
   "source": [
    "# Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f873f828-0bf0-4d13-a9f0-a0d8a55f5781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Stopwords & Punctuation ---\n",
      " ['Elon', 'Musk', 'CEO', 'Tesla']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens_clean = [w for w in tokens if w.lower() not in stop_words and w not in string.punctuation]\n",
    "print(\"\\n--- After Removing Stopwords & Punctuation ---\\n\", tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca621f3-3c06-4195-a184-98f85c7d6a78",
   "metadata": {},
   "source": [
    "# POS Tagging & NER using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c41ce-a2e8-47ac-bffd-3690ae14b440",
   "metadata": {},
   "source": [
    "## Load English model for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27e2864-dbba-4548-96b8-586eb4928032",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # Load small English model\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c0814-ce20-4fad-b150-c0659431712b",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eca2cb-f13c-4298-ae85-f681e4da203a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- POS Tags ---\n",
      " [('Elon', 'NNP'), ('Musk', 'NNP'), ('CEO', 'NNP'), ('Tesla', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens_clean)\n",
    "print(\"\\n--- POS Tags ---\\n\", pos_tags)\n",
    "\n",
    "# print(\"\\n--- POS Tagging ---\")\n",
    "# for token in doc:\n",
    "#     print(f\"{token.text:<12} --> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd0b5b-540a-4b7e-99ad-4b0a90035556",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635aff8c-e78b-4ae4-b526-5ce8a91d1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Named Entities ---\n",
      "Elon Musk -> PERSON\n",
      "Tesla -> ORG\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n--- Named Entities ---\")\n",
    "# for ent in doc.ents:\n",
    "#     print(f\"{ent.text:<15} --> {ent.label_}\")\n",
    "\n",
    "print(\"\\n--- Named Entities ---\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52815b6-61b7-40f4-9643-2b3349b455f9",
   "metadata": {},
   "source": [
    "# B: Bag of Words, TF-IDF Vectorisation & Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c61b19f-9ab7-493f-b381-a7a3bad679c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Apple is looking at buying a startup\",\n",
    "    \"Elon Musk is the CEO of Tesla\",\n",
    "    \"Tesla is building electric cars\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce06a5-3405-45c7-90a1-e1b37bd55478",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a761b3ae-f0d3-412e-b8f6-524d89488925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bag of Words ---\n",
      "['apple' 'at' 'building' 'buying' 'cars' 'ceo' 'electric' 'elon' 'is'\n",
      " 'looking' 'musk' 'of' 'startup' 'tesla' 'the']\n",
      "[[1 1 0 1 0 0 0 0 1 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 0 1 0 1 0 1 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "print(\"--- Bag of Words ---\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9903d867-fb2d-404e-a1b4-9f5bcea6ee12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bag of Words ---\n",
      "['ceo' 'elon' 'is' 'musk' 'of' 'tesla' 'the']\n",
      "[[1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow1 = vectorizer.fit_transform([text])\n",
    "print(\"--- Bag of Words ---\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667531ac-7813-4a69-b095-7b38e8fb5cd2",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a78545-4bad-45e2-b87b-6941b68266ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TF-IDF ---\n",
      "['apple' 'at' 'building' 'buying' 'cars' 'ceo' 'electric' 'elon' 'is'\n",
      " 'looking' 'musk' 'of' 'startup' 'tesla' 'the']\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"--- TF-IDF ---\")\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13fc0db-08d8-4b08-8607-9f2092e02cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TF-IDF ---\n",
      "['ceo' 'elon' 'is' 'musk' 'of' 'tesla' 'the']\n",
      "[[0.37796447 0.37796447 0.37796447 0.37796447 0.37796447 0.37796447\n",
      "  0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text])\n",
    "print(\"--- TF-IDF ---\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09910da6-65b9-45c9-af8d-db2bcd6f6e91",
   "metadata": {},
   "source": [
    "# N-grams (Bigrams and Trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d76f1-149b-4a92-81f0-635c5773fd13",
   "metadata": {},
   "source": [
    "## Bigrams (2-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd5f810-ad85-403f-9b72-00c693fd54ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for Tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk'),\n",
       " ('Musk', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'CEO'),\n",
       " ('CEO', 'of'),\n",
       " ('of', 'Tesla')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Bigrams for Tokens\")\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f47e13d3-9064-4d9f-899d-a452cfda990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for a text\n",
      "['ceo of' 'elon musk' 'is the' 'musk is' 'of tesla' 'the ceo']\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ng1 = ngram_vectorizer.fit_transform([text])\n",
    "print(\"Bigrams for a text\")\n",
    "print(ngram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe7744a-7c68-4277-ba5f-5ddbff773fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for 3 Sentence\n",
      "['apple is' 'at buying' 'building electric' 'buying startup' 'ceo of'\n",
      " 'electric cars' 'elon musk' 'is building' 'is looking' 'is the'\n",
      " 'looking at' 'musk is' 'of tesla' 'tesla is' 'the ceo']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))  # bigrams\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Bigrams for 3 Sentence\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c48c8d-e54d-47c6-9008-e122c8669468",
   "metadata": {},
   "source": [
    "## Trigrams (3-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "751b6462-387a-400a-af9d-0ea3f6f7f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams for Tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk', 'is'),\n",
       " ('Musk', 'is', 'the'),\n",
       " ('is', 'the', 'CEO'),\n",
       " ('the', 'CEO', 'of'),\n",
       " ('CEO', 'of', 'Tesla')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Trigrams for Tokens\")\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b64c9622-c8c6-4084-93eb-abc24c01db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams for a text\n",
      "['ceo of tesla' 'elon musk is' 'is the ceo' 'musk is the' 'the ceo of']\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "ng1 = ngram_vectorizer.fit_transform([text])\n",
    "print(\"Trigrams for a text\")\n",
    "print(ngram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8048b448-8cdc-42b1-be17-5988a19f950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams for 3 Sentence\n",
      "['apple is looking' 'at buying startup' 'building electric cars'\n",
      " 'ceo of tesla' 'elon musk is' 'is building electric' 'is looking at'\n",
      " 'is the ceo' 'looking at buying' 'musk is the' 'tesla is building'\n",
      " 'the ceo of']\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "ng1 = ngram_vectorizer.fit_transform(corpus)\n",
    "print(\"Trigrams for 3 Sentence\")\n",
    "print(ngram_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
